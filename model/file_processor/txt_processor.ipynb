{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TXT to JSON Processor (Unstructured Data)\n",
                "\n",
                "This notebook processes a TXT file, extracts its text, splits the text into sentences, performs data cleaning, and translation (if needed), and converts the result into a JSON file."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 1: Import Necessary Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install nltk contractions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "\n",
                "import nltk\n",
                "import contractions\n",
                "from nltk.tokenize import word_tokenize, sent_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "import tkinter as tk\n",
                "from tkinter import filedialog\n",
                "import sys\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
                "import model.language_manager.lang_detector as LanguageDetector\n",
                "import model.language_manager.lang_translator as LanguageTranslator\n",
                "import model.language_manager.lang_spelling as LanguageSpellChecker\n",
                "\n",
                "nltk.download('punkt')\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2: Load the TXT File"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "selected_file_path = None  \n",
                "\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "def select_file():\n",
                "    global selected_file_path\n",
                "    file_path = filedialog.askopenfilename(\n",
                "        title=\"Select a TXT file\",\n",
                "        filetypes=((\"Text Files\", \"*.txt\"), (\"All Files\", \"*.*\"))\n",
                "    )\n",
                "\n",
                "    if file_path:\n",
                "        selected_file_path = file_path\n",
                "        label.config(text=os.path.basename(file_path))  # Show only filename\n",
                "        with open(file_path, 'r', encoding='utf-8') as file:\n",
                "            content = file.read()\n",
                "            text_widget.delete(1.0, tk.END)\n",
                "            text_widget.insert(tk.END, content)\n",
                "        button_confirm.pack(pady=10)\n",
                "    else:\n",
                "        label.config(text=\"No file selected\")\n",
                "        button_confirm.pack_forget()\n",
                "\n",
                "def confirm_file():\n",
                "    window.destroy()  # Close the GUI\n",
                "\n",
                "# --- GUI SETUP ---\n",
                "window = tk.Tk()\n",
                "window.title(\"Select a TXT File\")\n",
                "window.geometry(\"700x400+100+100\")\n",
                "\n",
                "label = tk.Label(window, text=\"No file selected\", width=80)\n",
                "label.pack(pady=20)\n",
                "\n",
                "button_select = tk.Button(window, text=\"Select File\", command=select_file)\n",
                "button_select.pack(pady=10)\n",
                "\n",
                "button_confirm = tk.Button(window, text=\"Load File\", command=confirm_file)\n",
                "\n",
                "text_widget = tk.Text(window, width=80, height=10)\n",
                "text_widget.pack(pady=10)\n",
                "\n",
                "window.mainloop()\n",
                "\n",
                "# --- After GUI closes ---\n",
                "if selected_file_path:\n",
                "    print(f\"\\nConfirmed TXT file path: {selected_file_path}\")\n",
                "else:\n",
                "    print(\"\\nNo file selected.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3: Extract Text from the TXT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to extract text from TXT\n",
                "def extract_text_from_txt(txt_path):\n",
                "    text = \"\"\n",
                "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
                "        text = f.read()\n",
                "    return text\n",
                "\n",
                "# Extract text from the TXT\n",
                "text = extract_text_from_txt(selected_file_path)\n",
                "\n",
                "# Display a snippet of the extracted text\n",
                "print(\"Extracted Text (First 500 characters):\\n\")\n",
                "print(text[:500])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 4: Split Text into Sentences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Before sentence tokenization\n",
                "text = contractions.fix(text)\n",
                "\n",
                "# Split the text into sentences\n",
                "sentences = sent_tokenize(text)\n",
                "\n",
                "lines = text.strip().split('\\n')  # Split the entire text by line\n",
                "\n",
                "# Build structured sentence data from each line (Tag + Sentence)\n",
                "sentence_data = [\n",
                "    {\n",
                "        \"tag\": parts[0].upper(),\n",
                "        \"sentence\": parts[1].strip()\n",
                "    }\n",
                "    for i, line in enumerate(lines)\n",
                "    if (parts := line.strip().split(\" \", 1)) and len(parts) == 2\n",
                "]\n",
                "\n",
                "# Display the first few sentences\n",
                "print(\"Extracted Sentences:\")\n",
                "for entry in sentence_data[:5]:\n",
                "    print(f\"Tag: {entry['tag']}\\nSentence: {entry['sentence']}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 5: Perform Data Cleaning and Translation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize NLP tools\n",
                "stopwords_dict = {\"english\": set(stopwords.words(\"english\"))}\n",
                "lemmatizers = {\"english\": WordNetLemmatizer()}\n",
                "processed_sentences = set()\n",
                "cleaned_data = []\n",
                "\n",
                "def preserve_place_names(sentence):\n",
                "    \"\"\"\n",
                "    Detect consecutive title-cased words (likely proper nouns) and replace with placeholders.\n",
                "    \"\"\"\n",
                "    words = sentence.split()\n",
                "    placeholder_map = {}\n",
                "    new_words = []\n",
                "    i = 0\n",
                "    while i < len(words):\n",
                "        if words[i].istitle() and len(words[i]) > 2:\n",
                "            phrase = [words[i]]\n",
                "            j = i + 1\n",
                "            while j < len(words) and words[j].istitle() and len(words[j]) > 2:\n",
                "                phrase.append(words[j])\n",
                "                j += 1\n",
                "            if len(phrase) >= 1:\n",
                "                placeholder = f\"__PLACE_{len(placeholder_map)}__\"\n",
                "                original_phrase = \" \".join(phrase)\n",
                "                placeholder_map[placeholder] = original_phrase\n",
                "                new_words.append(placeholder)\n",
                "                i = j\n",
                "                continue\n",
                "        new_words.append(words[i])\n",
                "        i += 1\n",
                "    return \" \".join(new_words), placeholder_map\n",
                "\n",
                "\n",
                "def restore_place_names(sentence, placeholder_map):\n",
                "    for placeholder, original in placeholder_map.items():\n",
                "        sentence = sentence.replace(placeholder, original)\n",
                "    return sentence\n",
                "\n",
                "\n",
                "def clean_data(entry, idx, total):\n",
                "    sentence = entry[\"sentence\"].strip()\n",
                "    \n",
                "    if not sentence or sentence in processed_sentences:\n",
                "        return None\n",
                "\n",
                "    processed_sentences.add(sentence)\n",
                "\n",
                "    print(f\"\\nProcessing sentence {idx+1}/{total}:\")\n",
                "    print(f\"Original: {sentence}\")\n",
                "\n",
                "    # ‚¨áÔ∏è STEP 1: Detect Language FIRST\n",
                "    detected_lang = LanguageDetector.detect_language(sentence)\n",
                "    print(f\"Detected Language: {detected_lang}\")\n",
                "\n",
                "    # ‚¨áÔ∏è STEP 2: Preserve places BEFORE translation\n",
                "    if detected_lang != \"en\":\n",
                "        safe_sentence, placeholder_map = preserve_place_names(sentence)\n",
                "        translated = str(LanguageTranslator.translate_text(safe_sentence, \"en\"))\n",
                "    # ‚¨áÔ∏è Apply spelling correction to the TRANSLATED string with placeholders\n",
                "        translated = LanguageSpellChecker.correct_spelling(translated)\n",
                "        # ‚¨áÔ∏è Then restore original place names (so they aren't corrupted)\n",
                "        sentence = restore_place_names(translated, placeholder_map)\n",
                "        # üîÑ Expand contractions like it's ‚Üí it is\n",
                "        sentence = contractions.fix(sentence)\n",
                "        print(f\"Translated, corrected, and expanded: {sentence}\")\n",
                "    else:\n",
                "        sentence = LanguageSpellChecker.correct_spelling(sentence)\n",
                "        sentence = contractions.fix(sentence)\n",
                "        print(f\"Corrected & expanded: {sentence}\")\n",
                "\n",
                "    # üîß Tokenize after all corrections (applies to both translated and native English)\n",
                "    tokens = word_tokenize(sentence.lower())\n",
                "    filtered_tokens = [\n",
                "        word for word in tokens\n",
                "        if (\n",
                "            word.isalnum() or any(char in word for char in \"!@#$%^&*()-_=+[]{}|;:'\\\",.<>?\")\n",
                "        ) and word not in stopwords_dict[\"english\"]\n",
                "    ]\n",
                "\n",
                "\n",
                "    lemmatized_tokens = [lemmatizers[\"english\"].lemmatize(word) for word in filtered_tokens]\n",
                "\n",
                "    if not lemmatized_tokens:\n",
                "        print(\"Skipping sentence after lemmatization as it's empty.\")\n",
                "        return None\n",
                "\n",
                "    lemmatized_sentence = \" \".join(lemmatized_tokens)\n",
                "\n",
                "    if lemmatized_sentence in processed_sentences:\n",
                "        return None\n",
                "\n",
                "    processed_sentences.add(lemmatized_sentence)\n",
                "    entry[\"sentence\"] = lemmatized_sentence\n",
                "    print(f\"Lemmatized and processed sentence: {lemmatized_sentence}\")\n",
                "    return entry\n",
                "\n",
                "\n",
                "for idx, entry in enumerate(sentence_data):\n",
                "    result = clean_data(entry, idx, len(sentence_data)) \n",
                "\n",
                "    if result:\n",
                "        cleaned_data.append(result)\n",
                "\n",
                "# Replace original sentence_data with cleaned_data\n",
                "sentence_data = [entry for entry in cleaned_data if entry[\"sentence\"].strip()]\n",
                "\n",
                "print(\"Preprocessing completed.\\n\")\n",
                "print(\"Sample of cleaned and corrected sentences:\")\n",
                "for entry in sentence_data[:5]:\n",
                "    print(f\"[{entry['tag']}] {entry['sentence']}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 6: Save the Sentences to a JSON File"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "selected_filename = os.path.basename(selected_file_path) \n",
                "selected_filename_wo_ext = os.path.splitext(selected_filename)[0]\n",
                "\n",
                "# Now build a safe output path\n",
                "output_file_path = f\"../../data/processed/processed_{selected_filename_wo_ext}.json\"\n",
                "\n",
                "# Ensure the directory exists\n",
                "os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
                "\n",
                "# Save the sentences to a JSON file\n",
                "with open(output_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
                "    json.dump(sentence_data, json_file, indent=4, ensure_ascii=False)\n",
                "\n",
                "print(f\"Processed JSON data saved to {output_file_path}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### - END"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
