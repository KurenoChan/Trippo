{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PDF to JSON Processor (Unstructured Data)\n",
                "\n",
                "This notebook processes a PDF file, extracts its text (using OCR if necessary), splits the text into sentences, performs data cleaning, translation (if needed), and converts the result into a JSON file."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 1: Import Necessary Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install PyPDF2 pytesseract pdf2image nltk\n",
                "%pip install spacy\n",
                "!python -m spacy download en_core_web_sm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "from PyPDF2 import PdfReader\n",
                "from pdf2image import convert_from_path\n",
                "import pytesseract\n",
                "import contractions\n",
                "import nltk\n",
                "import tkinter as tk\n",
                "from tkinter import filedialog, messagebox\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from nltk.tokenize import sent_tokenize\n",
                "from nltk import WordNetLemmatizer\n",
                "import tkinter as tk\n",
                "import os\n",
                "import json\n",
                "import spacy\n",
                "nlp = spacy.load(\"en_core_web_sm\")\n",
                "\n",
                "import sys\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
                "import model.language_manager.lang_detector as LanguageDetector\n",
                "import model.language_manager.lang_translator as LanguageTranslator\n",
                "import model.language_manager.lang_spelling as LanguageSpellChecker\n",
                "\n",
                "nltk.download('punkt')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2: Load the PDF File"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tourism relevance settings\n",
                "tourism_keywords = [\"flight\", \"hotel\", \"trip\", \"tour\", \"travel\", \"tourist\"]\n",
                "relaxed_terms = {\n",
                "    \"day\", \"weather\", \"recommend\", \"place\", \"where\", \"when\", \"how much\", \"suggest\",\n",
                "    \"good\", \"beautiful\", \"scenery\", \"things to do\", \"what to see\", \"how to get\", \"trip\"\n",
                "}\n",
                "tourism_reference = [\n",
                "    \"travel\", \"tourism\", \"flight\", \"hotel\", \"vacation\", \"trip\", \n",
                "    \"tourist\", \"beach\", \"accommodation\", \"sightseeing\", \"cruise\", \"spot\"\n",
                "]\n",
                "tag_definitions = {\n",
                "\"TOURISM_ACTIVITY\": \"tour, visit, explore, sightseeing, activity, adventure\",\n",
                "\"ACCOMMODATION\": \"hotel, resort, hostel, stay, accommodation, lodge\",\n",
                "\"TRANSPORTATION\": \"flight, train, bus, transport, airport, travel\",\n",
                "\"DESTINATION\": \"beach, city, location, destination, attraction\",\n",
                "\"FOOD\": \"restaurant, local cuisine, dish, food, dining\",\n",
                "\"BOOKING\": \"book, reservation, schedule, availability, check-in\",\n",
                "\"COST\": \"price, fee, rate, budget, cost, charge\",\n",
                "\"SAFETY\": \"safe, emergency, secure, risk, health\",\n",
                "\"SEASONALITY\": \"weather, season, best time, climate\",\n",
                "\"CULTURE\": \"tradition, history, cultural, heritage, museum\"\n",
                "}\n",
                "\n",
                "tag_sentences = list(tag_definitions.values())\n",
                "tag_labels = list(tag_definitions.keys())\n",
                "\n",
                "tfidf_vectorizer = TfidfVectorizer()\n",
                "tag_vectors = tfidf_vectorizer.fit_transform(tag_sentences)\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "# --- GUI to select a PDF file ---\n",
                "selected_pdf_path = None\n",
                "\n",
                "def select_file():\n",
                "    global selected_pdf_path\n",
                "\n",
                "    path = filedialog.askopenfilename(\n",
                "        title=\"Select a PDF File\",\n",
                "        filetypes=[(\"PDF Files\", \"*.pdf\"), (\"All Files\", \"*.*\")]\n",
                "    )\n",
                "\n",
                "    if path:\n",
                "        selected_pdf_path = path\n",
                "        label.config(text=os.path.basename(path))\n",
                "        text_widget.delete(1.0, tk.END)\n",
                "        text_widget.insert(tk.END, f\"Selected File:\\n{selected_pdf_path}\")\n",
                "        button_confirm.pack(pady=10)\n",
                "    else:\n",
                "        label.config(text=\"No file selected\")\n",
                "        text_widget.delete(1.0, tk.END)\n",
                "        button_confirm.pack_forget()\n",
                "\n",
                "def confirm_file():\n",
                "    if selected_pdf_path:\n",
                "        window.destroy()\n",
                "    else:\n",
                "        messagebox.showwarning(\"No Selection\", \"Please select a file before confirming.\")\n",
                "\n",
                "# --- GUI Setup ---\n",
                "window = tk.Tk()\n",
                "window.title(\"Select a PDF File\")\n",
                "window.geometry(\"800x400+100+100\")\n",
                "\n",
                "label = tk.Label(window, text=\"No file selected\", width=100)\n",
                "label.pack(pady=20)\n",
                "\n",
                "button_select = tk.Button(window, text=\"Select File\", command=select_file)\n",
                "button_select.pack(pady=10)\n",
                "\n",
                "button_confirm = tk.Button(window, text=\"Load File\", command=confirm_file)\n",
                "\n",
                "text_widget = tk.Text(window, width=100, height=10)\n",
                "text_widget.pack(pady=10)\n",
                "\n",
                "window.mainloop()\n",
                "\n",
                "# --- After GUI closes ---\n",
                "if selected_pdf_path:\n",
                "    print(f\"\\nâœ… Confirmed PDF file path: {selected_pdf_path}\")\n",
                "else:\n",
                "    print(\"\\nâŒ No file selected.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Prompt for Page Range"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a PdfReader instance\n",
                "reader = PdfReader(selected_pdf_path)\n",
                "\n",
                "# Check the number of pages in the PDF\n",
                "num_pages = len(reader.pages)\n",
                "\n",
                "if num_pages > 1:\n",
                "    # Prompt the user for page range\n",
                "    page_range_input = input(\"Do you want to extract text from all pages? (Y/N): \").strip().lower()\n",
                "\n",
                "    if page_range_input == \"n\":\n",
                "        start_page = int(input(\"Enter the starting page number (must be at least 1): \"))\n",
                "        end_page = int(input(\"Enter the ending page number: \"))\n",
                "        \n",
                "        # Validate the page range\n",
                "        if start_page < 1:\n",
                "            raise ValueError(\"Starting page number must be at least 1.\")\n",
                "        if end_page < start_page:\n",
                "            raise ValueError(\"Ending page number must be greater than or equal to the starting page number.\")\n",
                "        if end_page > num_pages:\n",
                "            raise ValueError(f\"Ending page number must be at most {num_pages}.\")\n",
                "    else:\n",
                "        start_page, end_page = None, None\n",
                "else:\n",
                "    print(\"The PDF has only one page. Extracting text from the single page.\")\n",
                "    start_page, end_page = None, None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3: Extract Text from the PDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to extract text from PDF with optional page range\n",
                "def extract_text_from_pdf(pdf_path, start_page=None, end_page=None):\n",
                "    text = \"\"\n",
                "    reader = PdfReader(pdf_path)\n",
                "    \n",
                "    # Determine the range of pages to process\n",
                "    pages_to_process = reader.pages\n",
                "    if start_page is not None and end_page is not None:\n",
                "        pages_to_process = reader.pages[start_page - 1:end_page]\n",
                "    \n",
                "    # Extract text from the specified pages\n",
                "    for page in pages_to_process:\n",
                "        text += page.extract_text() or \"\"\n",
                "    return text\n",
                "\n",
                "# Extract text from the PDF based on the page range\n",
                "text = extract_text_from_pdf(selected_pdf_path, start_page, end_page)\n",
                "\n",
                "# If no text is extracted, use OCR\n",
                "if not text.strip():\n",
                "    print(\"No text found in the PDF. Attempting OCR...\")\n",
                "    images = convert_from_path(selected_pdf_path)\n",
                "    if start_page is not None and end_page is not None:\n",
                "        images = images[start_page - 1:end_page]\n",
                "    text = \"\\n\".join([pytesseract.image_to_string(image) for image in images])\n",
                "\n",
                "# Display a snippet of the extracted text\n",
                "print(\"Extracted Text (First 500 characters):\\n\")\n",
                "print(text[:500])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 4: Split Text into Sentences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split the text into sentences\n",
                "sentences = sent_tokenize(text)\n",
                "\n",
                "# Convert sentences into a DataFrame-like structure\n",
                "sentence_data = [{\"sentence\": sentence} for i, sentence in enumerate(sentences)]\n",
                "\n",
                "# Display the first few sentences\n",
                "print(\"Extracted Sentences:\")\n",
                "print(sentence_data[:5])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 5: Perform Data Cleaning and Translation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 5: Perform Data Cleaning and Translation\n",
                "stopwords_dict = {\"english\": set(stopwords.words(\"english\"))}\n",
                "lemmatizers = {\"english\": WordNetLemmatizer()}\n",
                "processed_sentences = set()\n",
                "cleaned_data = []\n",
                "\n",
                "def preserve_place_names(sentence):\n",
                "    doc = nlp(sentence)\n",
                "    placeholder_map = {}\n",
                "    for i, ent in enumerate(doc.ents):\n",
                "        if ent.label_ in {\"GPE\", \"LOC\", \"FAC\"}:  # GPE: countries, cities, states; LOC: non-GPE locations; FAC: buildings, airports, etc.\n",
                "            placeholder = f\"__PLACE_{i}__\"\n",
                "            original_text = ent.text\n",
                "            sentence = sentence.replace(original_text, placeholder)\n",
                "            placeholder_map[placeholder] = original_text\n",
                "    return sentence, placeholder_map\n",
                "def assign_tag(cleaned_text):\n",
                "    sentence_vector = tfidf_vectorizer.transform([cleaned_text])\n",
                "    similarities = cosine_similarity(sentence_vector, tag_vectors)\n",
                "    best_idx = similarities.argmax()\n",
                "    return tag_labels[best_idx]\n",
                "\n",
                "def restore_place_names(sentence, placeholder_map):\n",
                "    for placeholder, original in placeholder_map.items():\n",
                "        sentence = sentence.replace(placeholder, original)\n",
                "    return sentence\n",
                "\n",
                "\n",
                "def clean_data(entry, idx, total):\n",
                "    sentence = entry[\"sentence\"].strip()\n",
                "    if not sentence or sentence in processed_sentences or sentence.isdigit():\n",
                "        return None\n",
                "\n",
                "    print(f\"\\n[{idx+1}/{total}] Original: {sentence}\")\n",
                "    detected_lang = LanguageDetector.detect_language(sentence)\n",
                "\n",
                "    # ðŸ”’ Protect named entities before translation and spell correction\n",
                "    safe_sentence, placeholder_map = preserve_place_names(sentence)\n",
                "\n",
                "    if detected_lang != \"en\":\n",
                "        translated = str(LanguageTranslator.translate_text(safe_sentence, \"en\"))\n",
                "    else:\n",
                "        translated = safe_sentence\n",
                "\n",
                "    corrected = LanguageSpellChecker.correct_spelling(translated)\n",
                "    restored = restore_place_names(corrected, placeholder_map)\n",
                "\n",
                "    print(f\"â†’ Translated + corrected: {restored}\")\n",
                "    sentence = contractions.fix(restored)\n",
                "\n",
                "    # âœ‚ Tokenize and clean\n",
                "    tokens = word_tokenize(sentence.lower())\n",
                "    filtered = [w for w in tokens if w.isalnum() and w not in stopwords_dict[\"english\"]]\n",
                "    lemmatized = [lemmatizer.lemmatize(w) for w in filtered]\n",
                "\n",
                "    if len(lemmatized) < 3:\n",
                "        print(\"â†’ Skipped: too short after cleaning\")\n",
                "        return None\n",
                "\n",
                "    final = \" \".join(lemmatized)\n",
                "    if final in processed_sentences:\n",
                "        print(\"â†’ Skipped: duplicate\")\n",
                "        return None\n",
                "\n",
                "    processed_sentences.add(final)\n",
                "    print(f\"â†’ Final cleaned: {final}\")\n",
                "    return {\n",
                "        \"tag\": assign_tag(sentence),\n",
                "        \"sentence\": final\n",
                "    }\n",
                "\n",
                "\n",
                "# Run cleaning\n",
                "total = len(sentence_data)\n",
                "for idx, entry in enumerate(sentence_data):\n",
                "    result = clean_data(entry, idx, total)\n",
                "    if result:\n",
                "        cleaned_data.append(result)\n",
                "\n",
                "sentence_data = cleaned_data\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 6: Save the Sentences to a JSON File"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pdf_filename_only = os.path.splitext(os.path.basename(selected_pdf_path))[0]\n",
                "output_file_path = f\"../../data/processed/processed_{pdf_filename_only}.json\"\n",
                "\n",
                "# --- Ensure the output directory exists ---\n",
                "os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
                "\n",
                "# --- Save sentences to JSON file ---\n",
                "with open(output_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
                "    json.dump(sentence_data, json_file, indent=4, ensure_ascii=False)\n",
                "\n",
                "print(f\"âœ… Processed JSON data saved to {output_file_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### - END"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
