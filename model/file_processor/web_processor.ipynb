{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c854d7fa",
   "metadata": {},
   "source": [
    "Step 0: Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f85614",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bs4\n",
    "%pip install requests\n",
    "%pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import nltk\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import contractions\n",
    "import sys\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "import model.language_manager.lang_detector as LanguageDetector\n",
    "import model.language_manager.lang_translator as LanguageTranslator\n",
    "import model.language_manager.lang_spelling as LanguageSpellChecker\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stopwords_dict = {\"english\": set(stopwords.words(\"english\"))}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tag_definitions = {\n",
    "\"TOURISM_ACTIVITY\": \"tour, visit, explore, sightseeing, activity, adventure\",\n",
    "\"ACCOMMODATION\": \"hotel, resort, hostel, stay, accommodation, lodge\",\n",
    "\"TRANSPORTATION\": \"flight, train, bus, transport, airport, travel\",\n",
    "\"DESTINATION\": \"beach, city, location, destination, attraction\",\n",
    "\"FOOD\": \"restaurant, local cuisine, dish, food, dining\",\n",
    "\"BOOKING\": \"book, reservation, schedule, availability, check-in\",\n",
    "\"COST\": \"price, fee, rate, budget, cost, charge\",\n",
    "\"SAFETY\": \"safe, emergency, secure, risk, health\",\n",
    "\"SEASONALITY\": \"weather, season, best time, climate\",\n",
    "\"CULTURE\": \"tradition, history, cultural, heritage, museum\"\n",
    "}\n",
    "\n",
    "tag_sentences = list(tag_definitions.values())\n",
    "tag_labels = list(tag_definitions.keys())\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tag_vectors = tfidf_vectorizer.fit_transform(tag_sentences)\n",
    "\n",
    "# Tourism relevance setup\n",
    "tourism_keywords = [\"flight\", \"hotel\", \"trip\", \"tour\", \"travel\", \"vacation\", \"sightseeing\", \"beach\"]\n",
    "relaxed_terms = [\"place to stay\", \"book a trip\", \"tourist destination\"]\n",
    "tourism_reference = \" \".join(tourism_keywords + relaxed_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5856b7c",
   "metadata": {},
   "source": [
    "Step 1: Prompt the user for a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce1395",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 1: Input URL\")\n",
    "url = input(\"Enter the website URL: \").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10061280",
   "metadata": {},
   "source": [
    "Step 2: Fetch HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 2: Fetching webpage...\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()\n",
    "html_content = response.text\n",
    "print(\"Webpage fetched successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01aab7",
   "metadata": {},
   "source": [
    "Step 3: Parse the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 3: Parsing HTML...\")\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3610981",
   "metadata": {},
   "source": [
    "Step 4: Extracting paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2149dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [p.get_text(strip=True) for p in soup.find_all('p') if p.get_text(strip=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a83631",
   "metadata": {},
   "source": [
    "Step 5: Split paragraphs into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 5: Splitting into sentences...\")\n",
    "\n",
    "sentences = []\n",
    "for para in paragraphs:\n",
    "    sentences.extend(nltk.sent_tokenize(para))\n",
    "\n",
    "# âœ… Create structured data (index + sentence)\n",
    "sentence_data = [\n",
    "    {\"sentence\": sentence}\n",
    "    for i, sentence in enumerate(sentences)\n",
    "]\n",
    "\n",
    "print(f\" Extracted {len(sentence_data)} sentences.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700df574",
   "metadata": {},
   "source": [
    "Step6 clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a0af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Clean and Filter Sentences (Improved Version)\n",
    "processed_sentences = set()\n",
    "cleaned_sentences = []\n",
    "\n",
    "def preserve_place_names(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    placeholder_map = {}\n",
    "    for i, ent in enumerate(doc.ents):\n",
    "        if ent.label_ in {\"GPE\", \"LOC\", \"FAC\"}:  # GPE: countries, cities, states; LOC: non-GPE locations; FAC: buildings, airports, etc.\n",
    "            placeholder = f\"__PLACE_{i}__\"\n",
    "            original_text = ent.text\n",
    "            sentence = sentence.replace(original_text, placeholder)\n",
    "            placeholder_map[placeholder] = original_text\n",
    "    return sentence, placeholder_map\n",
    "\n",
    "def restore_place_names(sentence, placeholder_map):\n",
    "    for placeholder, original in placeholder_map.items():\n",
    "        sentence = sentence.replace(placeholder, original)\n",
    "    return sentence\n",
    "def assign_tag(cleaned_text):\n",
    "    sentence_vector = tfidf_vectorizer.transform([cleaned_text])\n",
    "    similarities = cosine_similarity(sentence_vector, tag_vectors)\n",
    "    best_idx = similarities.argmax()\n",
    "    return tag_labels[best_idx]\n",
    "def clean_data(entry, idx, total):\n",
    "    sentence = entry[\"sentence\"].strip()\n",
    "    if not sentence or sentence in processed_sentences or sentence.isdigit():\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n[{idx+1}/{total}] Original: {sentence}\")\n",
    "    detected_lang = LanguageDetector.detect_language(sentence)\n",
    "\n",
    "    # ðŸ”’ Protect named entities\n",
    "    safe_sentence, placeholder_map = preserve_place_names(sentence)\n",
    "\n",
    "    if detected_lang != \"en\":\n",
    "        translated = str(LanguageTranslator.translate_text(safe_sentence, \"en\"))\n",
    "    else:\n",
    "        translated = safe_sentence\n",
    "\n",
    "    corrected = LanguageSpellChecker.correct_spelling(translated)\n",
    "    restored = restore_place_names(corrected, placeholder_map)\n",
    "\n",
    "    print(f\"â†’ Translated + corrected: {restored}\")\n",
    "    sentence = contractions.fix(restored)\n",
    "\n",
    "    # âœ‚ Tokenize and clean\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    filtered = [w for w in tokens if w.isalnum() and w not in stopwords_dict[\"english\"]]\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in filtered]\n",
    "\n",
    "    if len(lemmatized) < 3:\n",
    "        print(\"â†’ Skipped: too short after cleaning\")\n",
    "        return None\n",
    "\n",
    "    final = \" \".join(lemmatized)\n",
    "    if final in processed_sentences:\n",
    "        print(\"â†’ Skipped: duplicate\")\n",
    "        return None\n",
    "\n",
    "    processed_sentences.add(final)\n",
    "    tag = assign_tag(final)\n",
    "    print(f\"â†’ Final cleaned: {final} [TAG: {tag}]\")\n",
    "    return {\"tag\": tag ,\"sentence\": final}\n",
    "\n",
    "\n",
    "# Run cleaning\n",
    "total = len(sentence_data)\n",
    "for idx, entry in enumerate(sentence_data):\n",
    "    result = clean_data(entry, idx, total)\n",
    "    if result:\n",
    "        cleaned_sentences.append(result)\n",
    "\n",
    "sentence_data = cleaned_sentences\n",
    "print(f\"âœ… Cleaned and retained {len(sentence_data)} tourism-related sentences.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6990a56",
   "metadata": {},
   "source": [
    "Step 6: Save to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eeedd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output directory\n",
    "output_dir = \"../../data/processed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Clean domain name and manually join the path using forward slashes\n",
    "domain = urlparse(url).netloc.replace('.', '_')\n",
    "output_file_path = f\"{output_dir}/processed_{domain}.json\"\n",
    "\n",
    "# Save structured list\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sentence_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Processed JSON data saved to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
