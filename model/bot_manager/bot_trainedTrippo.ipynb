{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "fa5b2041",
            "metadata": {},
            "source": [
                "# Building Conversational Chatbot Using Trained Models"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "1bc93ed3",
            "metadata": {},
            "source": [
                "### Step 1: Import Required Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b80af8de",
            "metadata": {},
            "outputs": [],
            "source": [
                "# install sentence_transformers if not already installed\n",
                "%pip install sentence_transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "89761746",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import joblib\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import tkinter as tk\n",
                "from tkinter import messagebox\n",
                "import re\n",
                "from pathlib import Path\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "import json\n",
                "from functools import lru_cache\n",
                "from IPython.display import display\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "import sys\n",
                "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
                "sys.path.append(project_root)\n",
                "import model.language_manager.lang_spelling as LanguageSpellingChecker"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "a6b88e2b",
            "metadata": {},
            "source": [
                "### Step 2: Load Models and Data"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "653c9173",
            "metadata": {},
            "source": [
                "##### 2.1 Set up paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1f447812",
            "metadata": {},
            "outputs": [],
            "source": [
                "script_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
                "models_path = os.path.join(script_dir, \"model/training_manager/models.pkl\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "713a7c52",
            "metadata": {},
            "source": [
                "##### 2.2 Load models and data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c8bd1555",
            "metadata": {},
            "outputs": [],
            "source": [
                "trained_models = joblib.load(models_path)\n",
                "print(f\"‚úÖ Loaded {len(trained_models)} trained models\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "c12aa890",
            "metadata": {},
            "source": [
                "##### 2.3 Check for loaded model details"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "59b6f7b3",
            "metadata": {},
            "outputs": [],
            "source": [
                "model_infos = []\n",
                "for result in trained_models:\n",
                "    model_infos.append({\n",
                "        \"Model\": result.get('model', 'No model available').__class__.__name__,\n",
                "        \"Predictor\": result.get('predictor', 'No predictor available'),\n",
                "        \"Features\": result.get('features', 'No features available'),\n",
                "        \"Description\": result.get('description', 'No description available')\n",
                "    })\n",
                "models_df = pd.DataFrame(model_infos)\n",
                "with pd.option_context('display.max_colwidth', None):\n",
                "    display(models_df)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "63f7ef0a",
            "metadata": {},
            "source": [
                "##### 2.4 Initialize embedder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "84d611c0",
            "metadata": {},
            "outputs": [],
            "source": [
                "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
                "print(\"‚úÖ Loaded sentence embedding model\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "28cdfeb4",
            "metadata": {},
            "source": [
                "### Step 3: Model Selection Logic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5aa04741",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "def select_model(user_input):\n",
                "    user_input_lower = user_input.lower().strip()\n",
                "    user_input_lower = LanguageSpellingChecker.correct_spelling(user_input_lower)\n",
                "    print(f\"\\nüîç Analyzing input: '{user_input_lower}'\")\n",
                "    \n",
                "    # --------------------------------------------------\n",
                "    # 1. First try exact keyword matching\n",
                "    # --------------------------------------------------\n",
                "    for model in trained_models:\n",
                "        # Match with predictor name (exact)\n",
                "        predictor = model.get('predictor', '').lower()\n",
                "        if predictor and predictor in user_input_lower:\n",
                "            print(f\"‚úÖ Exact predictor match: {predictor}\")\n",
                "            print(f\"‚≠ê Selected Model: {model.get('description', 'No description available')}\")\n",
                "            return model\n",
                "\n",
                "        # Cosine similarity on predictor\n",
                "        if embedder and predictor:\n",
                "            sim = cosine_similarity(\n",
                "                embedder.encode([user_input_lower]),\n",
                "                embedder.encode([predictor])\n",
                "            )[0][0]\n",
                "            if sim > 0.2:\n",
                "                print(f\"‚úÖ Predictor semantic match: '{predictor}' (similarity: {sim:.2f})\")\n",
                "                print(f\"‚≠ê Selected Model: {model.get('description', 'No description available')}\")\n",
                "                return model\n",
                "            \n",
                "        # Match with features (exact)\n",
                "        features = model.get('features', [])\n",
                "        matched_features = [feat for feat in features if feat.lower() in user_input_lower]\n",
                "        if matched_features:\n",
                "            print(f\"‚úÖ Feature match: {matched_features}\")\n",
                "            print(f\"‚≠ê Selected Model: {model.get('description', 'No description available')}\")\n",
                "            return model\n",
                "\n",
                "        # Cosine similarity on features\n",
                "        if embedder and features:\n",
                "            feature_sims = [\n",
                "                cosine_similarity(\n",
                "                    embedder.encode([user_input_lower]),\n",
                "                    embedder.encode([feat.lower()])\n",
                "                )[0][0] for feat in features\n",
                "            ]\n",
                "            max_sim = max(feature_sims) if feature_sims else 0\n",
                "            if max_sim > 0.2:\n",
                "                best_feat = features[feature_sims.index(max_sim)]\n",
                "                print(f\"‚úÖ Feature semantic match: '{best_feat}' (similarity: {max_sim:.2f})\")\n",
                "                print(f\"‚≠ê Selected Model: {model.get('description', 'No description available')}\")\n",
                "                return model\n",
                "    \n",
                "    # --------------------------------------------------\n",
                "    # 2. Special handling for tag classification models\n",
                "    # --------------------------------------------------\n",
                "    tag_models = [m for m in trained_models \n",
                "                if m.get('vectorizer') and m.get('predictor', '').lower() == 'tag']\n",
                "    \n",
                "    if tag_models and embedder:\n",
                "        tag_model = tag_models[0]  # Assuming one tag model\n",
                "        label_encoder = tag_model.get('label_encoder')\n",
                "        \n",
                "        if label_encoder:\n",
                "            # Get all possible tags\n",
                "            tag_classes = label_encoder.classes_\n",
                "            print(\"üè∑ Available tags (vertical):\")\n",
                "            for tag in tag_classes:\n",
                "                print(f\" - {tag}\")\n",
                "            \n",
                "            # Calculate similarity between input and each tag\n",
                "            tag_embeddings = embedder.encode(tag_classes)\n",
                "            user_embedding = embedder.encode([user_input_lower])\n",
                "            sims = cosine_similarity(user_embedding, tag_embeddings)[0]\n",
                "            \n",
                "            # Get best match\n",
                "            best_idx = np.argmax(sims)\n",
                "            best_tag = tag_classes[best_idx]\n",
                "            similarity = sims[best_idx]\n",
                "            \n",
                "            # Display semantic tag matching results in a DataFrame\n",
                "            tag_scores_df = pd.DataFrame({\n",
                "                'Tag': tag_classes,\n",
                "                'Similarity': sims\n",
                "            }).sort_values('Similarity', ascending=False).reset_index(drop=True)\n",
                "            display(tag_scores_df)\n",
                "            \n",
                "            # If we have a good match, return it directly\n",
                "            if similarity > 0.2:\n",
                "                print(f\"üè∑ Best semantic tag match: {best_tag} (similarity: {similarity:.2f})\")\n",
                "                print(f\"‚≠ê Selected Model: {tag_model.get('description', 'No description available')}\")\n",
                "                # Return special structure for tag matches\n",
                "                return {\n",
                "                    'model_type': 'semantic_tag_match',\n",
                "                    'matched_tag': best_tag,\n",
                "                    'similarity': similarity,\n",
                "                    'original_model': tag_model  # Keep reference to original model\n",
                "                }\n",
                "            else:\n",
                "                print(f\"‚ö† No strong tag match (best: {best_tag} @ {similarity:.2f})\")\n",
                "    \n",
                "    # --------------------------------------------------\n",
                "    # 3. Fallback to general model description matching\n",
                "    # --------------------------------------------------\n",
                "    if embedder:\n",
                "        print(\"üîé Trying general model matching...\")\n",
                "        try:\n",
                "            model_descriptions = []\n",
                "            for idx, model_info in enumerate(trained_models):\n",
                "                predictor = model_info.get('predictor', 'Unknown')\n",
                "                features = ', '.join(model_info.get('features', []))\n",
                "                desc = f\"Predicts {predictor} using {features}\"\n",
                "                model_descriptions.append(desc)\n",
                "            \n",
                "            description_embeddings = embedder.encode(model_descriptions)\n",
                "            user_embedding = embedder.encode([user_input_lower])\n",
                "            sims = cosine_similarity(user_embedding, description_embeddings)[0]\n",
                "            \n",
                "            # Get top 3 matches\n",
                "            top_indices = np.argsort(sims)[-3:][::-1]\n",
                "            \n",
                "            print(\"Top model matches:\")\n",
                "            for idx in top_indices:\n",
                "                print(f\" - #{idx}: {model_descriptions[idx]} (score: {sims[idx]:.2f})\")\n",
                "            \n",
                "            best_idx = top_indices[0]\n",
                "            if sims[best_idx] > 0.2:\n",
                "                print(f\"‚úÖ Selected model #{best_idx} (score: {sims[best_idx]:.2f})\")\n",
                "                return trained_models[best_idx]\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"‚ö† Model matching error: {e}\")\n",
                "    \n",
                "    # --------------------------------------------------\n",
                "    # 4. Final fallback\n",
                "    # --------------------------------------------------\n",
                "    print(\"‚ùå No suitable model found\")\n",
                "    return None"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "a89b10de",
            "metadata": {},
            "source": [
                "### Step 4: Enhanced Input Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e140f5a3",
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_input(model_info, user_input):\n",
                "    \"\"\"Prepare input data based on model type\"\"\"\n",
                "    if model_info.get('vectorizer'):\n",
                "        # Text classification model\n",
                "        return model_info['vectorizer'].transform([user_input])\n",
                "    else:\n",
                "        # Structured data model\n",
                "        features = {}\n",
                "        input_text = user_input.lower()\n",
                "        \n",
                "        # Handle one-hot encoded features\n",
                "        expected_columns = model_info.get('X_train_columns', [])\n",
                "        for col in expected_columns:\n",
                "            if '_' in col:  # One-hot encoded feature\n",
                "                feature, value = col.split('_', 1)\n",
                "                if feature.lower() in input_text and value.lower() in input_text:\n",
                "                    features[col] = 1\n",
                "                else:\n",
                "                    features[col] = 0\n",
                "            else:  # Regular feature\n",
                "                features[col] = 1 if col.lower() in input_text else 0\n",
                "        \n",
                "        # Display expected columns as a DataFrame (vertical format)\n",
                "        display(pd.DataFrame(expected_columns, columns=[\"Expected Feature\"]))\n",
                "        # Convert to DataFrame with correct column names\n",
                "        return pd.DataFrame([features], columns=expected_columns)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "3f7acaa8",
            "metadata": {},
            "source": [
                "### Step 5: Response Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e2a1cfb8",
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_response(model_info, user_input):\n",
                "    if not model_info:\n",
                "        return \"‚ö†Ô∏è No model found for this query\"\n",
                "    \n",
                "    # Handle direct tag matches\n",
                "    if 'matched_tag' in model_info:\n",
                "        return f\"üè∑Ô∏è Based on your query, the most relevant tag is: {model_info['matched_tag']} \" \\\n",
                "               f\"(match confidence: {model_info['similarity']:.0%})\"\n",
                "    \n",
                "    # Original model prediction logic\n",
                "    try:\n",
                "        model = model_info.get(\"model\")\n",
                "        if model is None:\n",
                "            return \"‚ö†Ô∏è No model available\"\n",
                "            \n",
                "        processed_input = prepare_input(model_info, user_input)\n",
                "        prediction = model.predict(processed_input)[0]\n",
                "        \n",
                "        if 'label_encoder' in model_info:\n",
                "            try:\n",
                "                prediction = model_info['label_encoder'].inverse_transform([prediction])[0]\n",
                "                print(f\"üìå Model predicted tag: {prediction}\")\n",
                "            except ValueError as e:\n",
                "                print(f\"‚ö†Ô∏è Label encoding error: {e}\")\n",
                "                return \"‚ö†Ô∏è Could not interpret the response\"\n",
                "        \n",
                "        confidence = 0.8\n",
                "        if hasattr(model, \"predict_proba\"):\n",
                "            proba = model.predict_proba(processed_input)\n",
                "            confidence = np.max(proba)\n",
                "            print(f\"üî¢ Confidence scores: {np.round(proba, 2)}\")\n",
                "        \n",
                "        return f\"‚úàÔ∏è The predicted {model_info.get('predictor', 'attribute')} is: {prediction} \" \\\n",
                "               f\"(confidence: {confidence:.0%})\"\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Error: {e}\")\n",
                "        return \"‚ö†Ô∏è Sorry, I encountered an error\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "688f875e",
            "metadata": {},
            "source": [
                "### Step 6: Chatbot GUI Interface"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1c5c624c",
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text(text):\n",
                "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation/symbols\n",
                "    text = re.sub(r'\\s+', ' ', text)     # Remove extra spaces\n",
                "    return text.strip().lower()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eedb0783",
            "metadata": {},
            "outputs": [],
            "source": [
                "def ask_question_gui():\n",
                "    def on_submit():\n",
                "        user_input = entry.get()        \n",
                "        cleaned_input = clean_text(user_input)\n",
                "        if not cleaned_input:\n",
                "            messagebox.showwarning(\"Missing Input\", \"Please ask a question.\")\n",
                "            return\n",
                "        \n",
                "        # Try model prediction\n",
                "        model_info = select_model(cleaned_input)\n",
                "        if model_info:\n",
                "            response = generate_response(model_info, cleaned_input)\n",
                "            if response:\n",
                "                messagebox.showinfo(\"TrippoBot Response\", response)\n",
                "                return\n",
                "        \n",
                "        # Fallback response\n",
                "        messagebox.showinfo(\"TrippoBot Response\", \n",
                "                          \"I couldn't find that information in my travel database. Could you try asking differently?\")\n",
                "\n",
                "    window = tk.Tk()\n",
                "    window.title(\"TrippoBot - Ask a Travel Question\")\n",
                "\n",
                "    tk.Label(window, text=\"Enter your travel question:\").pack(pady=10)\n",
                "    entry = tk.Entry(window, width=80)\n",
                "    entry.pack(padx=30, pady=15)\n",
                "\n",
                "    submit_btn = tk.Button(window, text=\"Ask TrippoBot\", command=on_submit)\n",
                "    submit_btn.pack(pady=10)\n",
                "\n",
                "    window.mainloop()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "26f2587e",
            "metadata": {},
            "outputs": [],
            "source": [
                "ask_question_gui()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "accfb1cd",
            "metadata": {},
            "source": [
                "### Step 7: Extract metrics from trained models into a structured format"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "56431ef7",
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_model_metrics(trained_models):\n",
                "    \"\"\"Extract metrics from trained models into a structured format\"\"\"\n",
                "    metrics_list = []\n",
                "    \n",
                "    for model_info in trained_models:\n",
                "        metrics = model_info['metrics']\n",
                "        model_data = {\n",
                "            'Model': model_info['model'].__class__.__name__,\n",
                "            'Task': f\"Predict {model_info['predictor']} using {', '.join(model_info['features'])}\",\n",
                "            'Accuracy': metrics.get('accuracy', np.nan),\n",
                "            'F1_Score': metrics.get('f1', np.nan),\n",
                "            'Precision': metrics.get('precision', np.nan),\n",
                "            'Recall': metrics.get('recall', np.nan),\n",
                "            'ROC_AUC': metrics.get('roc_auc', np.nan),\n",
                "            'Avg_Confidence': np.mean(metrics.get('confidence', [np.nan])) * 100 if metrics.get('confidence') is not None else np.nan\n",
                "        }\n",
                "        metrics_list.append(model_data)\n",
                "    \n",
                "    return pd.DataFrame(metrics_list)\n",
                "\n",
                "# Load your trained models\n",
                "try:\n",
                "    trained_models = joblib.load(models_path)\n",
                "    metrics_df = extract_model_metrics(trained_models)\n",
                "    \n",
                "    print(\"\\nMODEL PERFORMANCE METRICS\")\n",
                "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None, 'display.max_colwidth', None):\n",
                "        display(metrics_df)\n",
                "    \n",
                "except FileNotFoundError:\n",
                "    print(f\"Error: File not found at {models_path}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error: {str(e)}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "05cc7f60",
            "metadata": {},
            "source": [
                "### Step 8: Create Dynamic Visualizations\n",
                "##### Comparison Bar Chart"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c5a77553",
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_model_comparison(metrics_df):\n",
                "    \"\"\"Create a dynamic comparison plot based on available metrics\"\"\"\n",
                "    # Select only numeric columns for comparison\n",
                "    numeric_cols = metrics_df.select_dtypes(include=[np.number]).columns.tolist()\n",
                "    \n",
                "    # Set up plot\n",
                "    fig, axes = plt.subplots(nrows=len(numeric_cols), figsize=(10, 5*len(numeric_cols)), squeeze=False)\n",
                "    fig.suptitle('Model Performance Comparison', y=1.02)\n",
                "    \n",
                "    for idx, metric in enumerate(numeric_cols):\n",
                "        ax = axes[idx][0]\n",
                "        x = np.arange(len(metrics_df))\n",
                "        width = 0.35\n",
                "        \n",
                "        # Get values and convert to percentages where appropriate\n",
                "        values = metrics_df[metric].values\n",
                "        if metric != 'Avg_Confidence':  # Most metrics are already in 0-1 scale\n",
                "            values = [v * 100 for v in values]\n",
                "        \n",
                "        bars = ax.bar(x, values, width)\n",
                "        ax.set_title(metric.replace('_', ' '))\n",
                "        ax.set_ylabel('Score (%)')\n",
                "        ax.set_xticks(x)\n",
                "        ax.set_xticklabels(metrics_df['Model'])\n",
                "        ax.set_ylim(0, 105)\n",
                "        \n",
                "        # Add value labels\n",
                "        for bar in bars:\n",
                "            height = bar.get_height()\n",
                "            ax.annotate(f'{height:.1f}',\n",
                "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
                "                        xytext=(0, 3),\n",
                "                        textcoords=\"offset points\",\n",
                "                        ha='center', va='bottom')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "plot_model_comparison(metrics_df)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "ab10017d",
            "metadata": {},
            "source": [
                "##### Radar Chart for Comprehensive Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2a7dad95",
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_radar_comparison(metrics_df):\n",
                "    \"\"\"Create a radar chart comparing all models\"\"\"\n",
                "    # Select and normalize metrics\n",
                "    plot_metrics = ['Accuracy', 'F1_Score', 'Precision', 'Recall', 'ROC_AUC', 'Avg_Confidence']\n",
                "    plot_df = metrics_df.set_index('Model')[plot_metrics]\n",
                "    \n",
                "    # Normalize to 0-100 scale\n",
                "    plot_df = plot_df.apply(lambda x: x*100 if x.name != 'Avg_Confidence' else x)\n",
                "    \n",
                "    # Number of variables we're plotting\n",
                "    categories = list(plot_df.columns)\n",
                "    N = len(categories)\n",
                "    \n",
                "    # Calculate angle for each axis\n",
                "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
                "    angles += angles[:1]  # Complete the loop\n",
                "    \n",
                "    # Initialize the radar plot\n",
                "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
                "    \n",
                "    # Draw one axe per variable and add labels\n",
                "    plt.xticks(angles[:-1], categories)\n",
                "    \n",
                "    # Draw ylabels\n",
                "    ax.set_rlabel_position(0)\n",
                "    plt.yticks([20, 40, 60, 80, 100], [\"20\", \"40\", \"60\", \"80\", \"100\"], color=\"grey\", size=7)\n",
                "    plt.ylim(0, 100)\n",
                "    \n",
                "    # Plot each model\n",
                "    for idx, row in plot_df.iterrows():\n",
                "        values = row.values.flatten().tolist()\n",
                "        values += values[:1]  # Complete the loop\n",
                "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=idx)\n",
                "        ax.fill(angles, values, alpha=0.1)\n",
                "    \n",
                "    # Add legend and title\n",
                "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
                "    plt.title('Model Performance Radar Chart', size=14, y=1.1)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "plot_radar_comparison(metrics_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cc648b91",
            "metadata": {},
            "source": [
                "### - END"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
